{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for scraping the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionInfo(raw_data, url):\n",
    "    \n",
    "    session_info = dict()\n",
    "    \n",
    "    date = raw_data.find(\"input\", {\"id\": \"date\"}).get(\"value\").replace(\"/\", \"-\")\n",
    "    session_info['date'] = date\n",
    "    \n",
    "    keys = ['term', 'ejlasie', 'preset_mps', 'session']\n",
    "\n",
    "    info_string = raw_data.find(\"div\", {\"class\": \"col-sm-4 paddingHR\"}).text.strip() + \\\n",
    "                        \" - \" + raw_data.find(\"div\", {\"class\": \"col-sm-3 paddingHL\"}).text.strip()\n",
    "    \n",
    "    info_list = re.sub(\"\\\\r\\\\n +\", \" - \", info_string).split(\" - \")\n",
    "\n",
    "    for item, key in zip(info_list, keys):\n",
    "        session_info[key] = re.findall(r\"([0-9]+)\", item)[0]\n",
    "        \n",
    "    session_info['url'] = url \n",
    "\n",
    "    \n",
    "    return session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "    \n",
    "    \n",
    "    # Read data, Create soup object & Filter data\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    raw_data = BeautifulSoup(page.content, 'html.parser')\n",
    "    results = raw_data.findAll(\"div\", {\"class\": \"col-sm-10\"})\n",
    "    \n",
    "    \n",
    "    # Create dictionary & Insert `sessionInfo` into dictionary\n",
    "    \n",
    "    extracted = dict()\n",
    "    extracted[\"sessionInfo\"] = sessionInfo(raw_data, url)\n",
    "    \n",
    "    \n",
    "    # Extract `orders` and `discussions`\n",
    "    \n",
    "    content = []\n",
    "\n",
    "    for line in results:\n",
    "        if line.find(\"i\", {\"class\": \"fa\"}) != None:\n",
    "            content.append([line.text.strip()])\n",
    "        \n",
    "        elif line.find(\"div\", {\"class\": \"personInfoCol\"}) != None:\n",
    "            name = line.find(\"div\", {\"class\": \"personInfoCol\"}).find(\"div\", {\"class\": \"personInfoCol\"}).text.strip()\n",
    "            if line.find(\"div\", {\"class\": \"col-sm-10 colLeft\"}) != None:\n",
    "                opinion = line.find(\"div\", {\"class\": \"col-sm-10 colLeft\"}).find(\"div\", {\"class\": \"lector\"}).text.strip()\n",
    "            lecture = line.find(\"div\", {\"class\": \"lectorText\"}).text.strip()\n",
    "            content.append([name, opinion, lecture])\n",
    "    \n",
    "    \n",
    "    # Extract `orders` and `discussions` and insert into dictionary\n",
    "    \n",
    "    Mashrooh = dict()\n",
    "\n",
    "    start = 0\n",
    "    end = 1\n",
    "\n",
    "    for i in content[1:]:\n",
    "        if len(i) == 1:\n",
    "            end = content.index(i)\n",
    "            Mashrooh[\"order\"+content[start][0].split()[0]] = {\"title\": content[start][0], \"discussions\": content[start+1:end]}\n",
    "            start = end\n",
    "\n",
    "    Mashrooh[\"order\"+content[start][0].split()[0]] = {\"title\": content[start][0], \"discussions\": content[start+1:]}\n",
    "\n",
    "    extracted['mashrooh'] = Mashrooh\n",
    "    \n",
    "    \n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for exporting extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(extracted_data):\n",
    "    \n",
    "    term = extracted_data['sessionInfo']['term']\n",
    "    ejlasie = extracted_data['sessionInfo']['ejlasie']\n",
    "    session = extracted_data['sessionInfo']['session']\n",
    "    date = extracted_data['sessionInfo']['date']\n",
    "    \n",
    "    filename = f\"{date}_Term-{term}_Session-{session}_Ejlasie-{ejlasie}.json\"\n",
    "    \n",
    "    parsed = json.dumps(extracted_data, ensure_ascii=False)\n",
    "    \n",
    "    with open(\"exports/\"+filename, 'w', encoding='utf8') as json_file:\n",
    "        json.dump(parsed, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Valid session URLs from `valid_session_urls.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"valid_session_urls.csv\")[\"url\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped and exported 82 out of 82 page contents.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    extracted = scrape_url(url)\n",
    "    export_to_file(extracted)\n",
    "    print(f\"Scraped and exported {urls.index(url)+1} out of {len(urls)} page contents.\", end='\\r')\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
